layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
remotes::install_github("rstudio/tensorflow")
library(tensorflow)
install.packages("keras")
library(keras)
install_keras()
library(tensorflow)
tf$constant("Hello TensorFlow!")
detach("package:keras3", unload=TRUE)
detach("package:keras", unload=TRUE)
library(tensorflow)
tf$constant("Hello TensorFlow!")
library(tensorflow)
tf$constant("Hello TensorFlow!")
# library(keras3)
input_shape = c(50,50,3)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
mnist_all = readRDS("data/mnist.Rds")
class(mnist_all)
str(mnist_all)
mnist <- mnist_all
mnist$train$x <- mnist$train$x[1:1000,,]
mnist$train$y <- mnist$train$y[1:1000]
mnist$test$x <- mnist$test$x[1:1000,,]
mnist$test$y <- mnist$test$y[1:1000]
dim(mnist$train$x)
library(EBImage)
imgList = apply(mnist$train$x[1:10,,], 1, EBImage::Image, simplify = FALSE)
plot(EBImage::combine(lapply(imgList, t)), all = TRUE, nx = 5)
mnist$train$y[1:10]
X_train = apply(mnist$train$x[,,], 1, c) |> t()
dim(X_train)
X_train_no <- X_train[, colMeans(X_train) > 0]
pc = princomp(X_train_no, cor = TRUE)
df = data.frame(
PC1 = pc$scores[,1],
PC2 = pc$scores[,2],
PC3 = pc$scores[,3],
PC4 = pc$scores[,4],
y = factor(mnist$train$y)
)
library(ggplot2)
ggplot(df, aes(x = PC1, y = PC2)) +
geom_point(aes(colour = y)) +
theme_classic() +
NULL
ggplot(df, aes(x = PC2, y = PC3)) +
geom_point(aes(colour = y)) +
theme_classic() +
NULL
ggplot(df, aes(x = PC3, y = PC4)) +
geom_point(aes(colour = y)) +
theme_classic() +
NULL
library(OpenImageR)
Xe_train = apply(mnist$train$x, 1, HOG,
cells = 8, orientations = 9) |> t()
dim(Xe_train)
Xe_train_no <- Xe_train[, colMeans(Xe_train) > 0]
pc = princomp(Xe_train_no, cor = TRUE)
# library(keras3)
input_shape = c(50,50,3)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = "binary_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
?k_clear_session
library(keras3)
library(keras)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
library(tensorflow)
library(keras)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
# library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
# library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
install_keras(Tensorflow = "1.13.1",
restart_session = FALSE
)
# library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
install.packages("keras3")
library(keras3)
# library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
# library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
?k_clear_session
??k_clear_session
# library(keras3)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
keras::k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
# library(keras3)
library(tensorflow)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
keras::k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
# library(keras3)
library(tensorflow)
# library(keras3)
library(tensorflow)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
keras::k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
library(tensorflow)
input_shape = c(28,28,1)
model_function <- function(learning_rate = 0.001) {
keras::k_clear_session()
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3),
activation = NULL, input_shape = input_shape) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = NULL) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>% layer_flatten() %>%
layer_dense(units = 128, activation = NULL) %>%
layer_dropout(rate = 0.5) %>% layer_dense(units = 64) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 10, activation = 'softmax')
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_adam(learning_rate = learning_rate),
metrics = "accuracy"
)
return(model)
}
model <- model_function()
library(tensorflow)
tf$constant("Hellow Tensorflow")
tensorboard()
?choose
a = 1:6
b = 1:3
x = c()
for (i in a){
x[i] = x = choose(i,6)
}
sum(x)
a = 1:6
b = 1:3
x = c()
for (i in a){
x[i] = choose(i,6)
}
sum(x)
choose(i,6)
i
a = 1:6
b = 1:3
x = c()
for (i in a){
x[i] = choose(i,6)
}
x
a
a = 1:6
b = 1:3
x = c()
for (i in a){
x[i] = choose(6,i)
}
sum(x)
b = 1:3
x = c()
for (i in b){
x[i] = choose(6,i)
}
sum(x)
b = 1:3
x = c()
for (i in b){
x[i] = choose(3,i)
}
sum(x)
0.6 * 0.3 * 0.2
0.0324/0.036
0.4*0.2 + 0.6 * 0.75
0.75 * 0.2
0.2 * 0.2
0.4/100
26*102000 - 0.004*(102000) * 26
(26*102000 - 0.004*(102000) * 26) - (21*102000 - 0.004*(102000) * 21)
(26*102000 - 0.004*(102000) * 26) - (21*102000 - 0.004*(102000) * 21)
(25*96000 - 0.005*(96000) * 25) - (20*96000 - 0.005*(96000) * 20)
(24*96000 - 0.006*(113000) * 24) - (19*113000 - 0.006*(113000) * 19)
(22*132000 - 0.009*(132000) * 22) - (18*132000 - 0.009*(132000) * 18)
(26*102000 - 0.004*(102000) * 26) - (21*102000 - 0.004*(102000) * 21)
(25*96000 - 0.005*(96000) * 25) - (20*96000 - 0.005*(96000) * 20)
(24*96000 - 0.006*(113000) * 24) - (19*113000 - 0.006*(113000) * 19)
(22*132000 - 0.009*(132000) * 22) - (18*132000 - 0.009*(132000) * 18)
factorial(6) + factorial(3)
a = 1:9
b = 1:3
x = c()
for (i in a){
x[i] = choose(9,i)
}
sum(x)
setwd("~/Library/CloudStorage/OneDrive-Personal/PhD/Exclusion Automatic Threshold Selection")
setwd("~/Library/CloudStorage/OneDrive-Personal/PhD/Exclusion Automatic Threshold Selection/Automatic Threshold Selection")
citation("tidyverse")
citation("nvcreg")
citation("ncvreg")
citation("Rcpp")
citation("lars")
citation("stabs")
citation("knockoff")
citation("hdi")
citation("mvtnorm")
citation("latex2exp")
citation("patchwork")
citation("hrbrthemes")
citation("pbapply")
